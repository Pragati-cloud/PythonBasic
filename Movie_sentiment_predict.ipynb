{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRXOVnagLBUJOBSzLHG3ke",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pragati-cloud/PythonBasic/blob/main/Movie_sentiment_predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmlLWE9zQifo",
        "outputId": "204ef57a-0536-4ea3-b916-032dd2f4a708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m126.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.19.0 requires tensorflow<2.20,>=2.19, but you have tensorflow 2.20.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, but you have tensorflow 2.20.0 which is incompatible.\n",
            "tensorflow-text 2.19.0 requires tensorflow<2.20,>=2.19.0, but you have tensorflow 2.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-addons (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-addons\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# runtime: GPU recommended (Runtime -> Change runtime type -> GPU)\n",
        "!nvidia-smi\n",
        "\n",
        "# Install libraries\n",
        "!pip install -q tensorflow==2.20.0 pandas scikit-learn nltk tqdm requests\n",
        "# optional: tensorflow-addons might be used as fallback for AdamW\n",
        "!pip install -q tensorflow-addons\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Optional: set a path in your Drive where models/tokenizer will be saved\n",
        "DRIVE_OUT = \"/content/drive/MyDrive/sentiment_imdb\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "CH_h4TKwpXq3",
        "outputId": "7f964247-6ebf-4337-8fa4-0a83733de24a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4108790822.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Optional: set a path in your Drive where models/tokenizer will be saved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mDRIVE_OUT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/sentiment_imdb\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # use the file browser to upload your CSV (select the Kaggle IMDB CSV)\n",
        "# after upload, check filename\n",
        "for fn in uploaded.keys():\n",
        "    print(\"Uploaded:\", fn)\n",
        "DATA_CSV_PATH = list(uploaded.keys())[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "Fuj3DDI2pjir",
        "outputId": "08b0e786-c53b-404b-e103-24b6597df0ca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3b249744-270f-4de1-96fa-53f9d55dcf05\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3b249744-270f-4de1-96fa-53f9d55dcf05\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving IMDB Dataset.csv.zip to IMDB Dataset.csv.zip\n",
            "Uploaded: IMDB Dataset.csv.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, io, zipfile, requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "import pickle\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "ps = PorterStemmer()\n",
        "STOPWORDS = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3KPDC9qpxjs",
        "outputId": "ff673ec0-85ea-48d6-c783-70e93babd14f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(text):\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<.*?>', ' ', text)           # remove html\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text) # remove urls\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)        # non-letter chars -> space\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()     # collapse spaces\n",
        "    tokens = [ps.stem(tok) for tok in text.split() if tok not in STOPWORDS]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Hyperparams (you can tweak)\n",
        "MODEL_DIR = \"/content/model\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "VOCAB_SIZE = 20000\n",
        "MAX_LEN = 300\n",
        "EMBED_DIM = 100   # your chosen 100d\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "TEST_SIZE = 0.2\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "GLOVE_URL = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "GLOVE_DIR = \"/content/glove\"\n",
        "os.makedirs(GLOVE_DIR, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "P9lpL6PZqQJi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading CSV:\", DATA_CSV_PATH)\n",
        "df = pd.read_csv(DATA_CSV_PATH)\n",
        "\n",
        "# Accept common variants for column names\n",
        "if 'review' not in df.columns or 'sentiment' not in df.columns:\n",
        "    if 'text' in df.columns and 'label' in df.columns:\n",
        "        df = df.rename(columns={'text': 'review', 'label': 'sentiment'})\n",
        "    elif 'review' in df.columns and 'label' in df.columns:\n",
        "        df = df.rename(columns={'label': 'sentiment'})\n",
        "    else:\n",
        "        print(\"Columns found:\", df.columns)\n",
        "        raise ValueError(\"CSV must contain 'review' and 'sentiment' columns (or 'text'/'label').\")\n",
        "\n",
        "# Map textual sentiments ('positive'/'negative') to 1/0 if needed\n",
        "if df['sentiment'].dtype == object:\n",
        "    df['sentiment'] = df['sentiment'].str.strip().str.lower()\n",
        "    mapping = {'positive': 1, 'negative': 0, 'pos': 1, 'neg': 0}\n",
        "    # if the column contains 'positive'/'negative' strings, map them\n",
        "    if df['sentiment'].isin(['positive','negative','pos','neg']).all():\n",
        "        df['sentiment'] = df['sentiment'].map(mapping)\n",
        "    else:\n",
        "        # try to convert to numeric (handles '1'/'0' strings)\n",
        "        df['sentiment'] = pd.to_numeric(df['sentiment'], errors='coerce')\n",
        "\n",
        "# final sanity checks\n",
        "if df['sentiment'].isnull().any():\n",
        "    raise ValueError(\"Some sentiment labels couldn't be converted to ints. Unique values: \"\n",
        "                     + str(df['sentiment'].unique()))\n",
        "\n",
        "df['sentiment'] = df['sentiment'].astype(int)\n",
        "print(\"Loaded dataset size:\", len(df))\n",
        "print(\"Label distribution:\\n\", df['sentiment'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2V3eI3-yqUeC",
        "outputId": "fc455012-2798-4603-d769-7ec2556c30ac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading CSV: IMDB Dataset.csv.zip\n",
            "Loaded dataset size: 50000\n",
            "Label distribution:\n",
            " sentiment\n",
            "1    25000\n",
            "0    25000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cleaning reviews (may take a few minutes)...\")\n",
        "df['clean_review'] = df['review'].astype(str).apply(custom_standardization)\n",
        "\n",
        "X = df['clean_review'].values\n",
        "y = df['sentiment'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(X_train), \"Test size:\", len(X_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obsd_5DPqYBI",
        "outputId": "98feb4be-9256-4e50-b047-d92fa21cbb45"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning reviews (may take a few minutes)...\n",
            "Train size: 40000 Test size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# save tokenizer\n",
        "with open(os.path.join(MODEL_DIR, \"tokenizer.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "print(\"Tokenizer saved to\", os.path.join(MODEL_DIR, \"tokenizer.pkl\"))\n",
        "\n",
        "train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "X_test_pad = pad_sequences(test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "print(\"Padded shapes:\", X_train_pad.shape, X_test_pad.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQAS7pLxqbCV",
        "outputId": "a175557a-f4e1-489d-88f9-aee7d2efe26f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer saved to /content/model/tokenizer.pkl\n",
            "Padded shapes: (40000, 300) (10000, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile, requests, io\n",
        "glove_target = os.path.join(GLOVE_DIR, f\"glove.6B.{EMBED_DIM}d.txt\")\n",
        "\n",
        "if not os.path.exists(glove_target):\n",
        "    print(\"Downloading GloVe... (~862MB zip, will extract only the chosen file)\")\n",
        "    r = requests.get(GLOVE_URL, stream=True)\n",
        "    r.raise_for_status()\n",
        "    total = int(r.headers.get('content-length', 0))\n",
        "    buf = io.BytesIO()\n",
        "    for chunk in tqdm(r.iter_content(chunk_size=1024*1024), total=total//(1024*1024), desc=\"Downloading\"):\n",
        "        if chunk:\n",
        "            buf.write(chunk)\n",
        "    buf.seek(0)\n",
        "    with zipfile.ZipFile(buf) as z:\n",
        "        member = f\"glove.6B.{EMBED_DIM}d.txt\"\n",
        "        print(\"Extracting\", member)\n",
        "        z.extract(member, path=GLOVE_DIR)\n",
        "\n",
        "print(\"GloVe file at:\", glove_target)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeo6rvSgqwJ4",
        "outputId": "a0523389-9698-4074-ce39-f64f79dbd191"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading GloVe... (~862MB zip, will extract only the chosen file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 823it [02:39,  5.18it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting glove.6B.100d.txt\n",
            "GloVe file at: /content/glove/glove.6B.100d.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_path = os.path.join(GLOVE_DIR, f\"glove.6B.{EMBED_DIM}d.txt\")\n",
        "emb_index = {}\n",
        "print(\"Loading GloVe vectors (this may take a minute)...\")\n",
        "with open(glove_path, 'r', encoding='utf8') as f:\n",
        "    for line in tqdm(f, desc=\"Reading GloVe\"):\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vec = np.asarray(values[1:], dtype='float32')\n",
        "        emb_index[word] = vec\n",
        "print(\"GloVe vectors:\", len(emb_index))\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "num_tokens = min(VOCAB_SIZE, len(word_index) + 1)\n",
        "embedding_matrix = np.zeros((num_tokens, EMBED_DIM), dtype=np.float32)\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i >= num_tokens:\n",
        "        continue\n",
        "    vec = emb_index.get(word)\n",
        "    if vec is not None:\n",
        "        embedding_matrix[i] = vec\n",
        "    else:\n",
        "        embedding_matrix[i] = np.random.normal(scale=0.6, size=(EMBED_DIM,))\n",
        "\n",
        "print(\"Embedding matrix shape:\", embedding_matrix.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cjr8uUyVqzcM",
        "outputId": "22754b8e-c72a-4f10-a07e-24e62de1e2fb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GloVe vectors (this may take a minute)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading GloVe: 400000it [00:12, 33062.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GloVe vectors: 400000\n",
            "Embedding matrix shape: (20000, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(num_tokens, EMBED_DIM,\n",
        "              embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "              input_length=MAX_LEN, trainable=False),\n",
        "    SpatialDropout1D(0.3),\n",
        "    Bidirectional(LSTM(128, return_sequences=False)),\n",
        "    Dropout(0.4),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# AdamW fallback logic (you requested AdamW)\n",
        "try:\n",
        "    from tensorflow.keras.optimizers import AdamW\n",
        "    opt = AdamW(learning_rate=1e-3)\n",
        "    print(\"Using tf.keras.optimizers.AdamW\")\n",
        "except Exception:\n",
        "    try:\n",
        "        import tensorflow_addons as tfa\n",
        "        opt = tfa.optimizers.AdamW(weight_decay=1e-5, learning_rate=1e-3)\n",
        "        print(\"Using tensorflow_addons.AdamW\")\n",
        "    except Exception:\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "        print(\"AdamW not found; using Adam as fallback\")\n",
        "\n",
        "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "TJVTcZlsrIS7",
        "outputId": "c407aa1a-31ad-4fe9-fed4-2b6cfe587eba"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using tf.keras.optimizers.AdamW\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ spatial_dropout1d               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ spatial_dropout1d               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = os.path.join(MODEL_DIR, \"sentiment_model_best.keras\")\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1),\n",
        "    ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_pad, y_train,\n",
        "    validation_split=0.1,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hG1c9-VCrwqN",
        "outputId": "393042f5-b590-4ef6-b42d-735c5f66f6c5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6031 - loss: 0.6568\n",
            "Epoch 1: val_loss improved from inf to 0.57331, saving model to /content/model/sentiment_model_best.keras\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 2s/step - accuracy: 0.6032 - loss: 0.6567 - val_accuracy: 0.7180 - val_loss: 0.5733 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6921 - loss: 0.5936\n",
            "Epoch 2: val_loss improved from 0.57331 to 0.49566, saving model to /content/model/sentiment_model_best.keras\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 2s/step - accuracy: 0.6921 - loss: 0.5935 - val_accuracy: 0.7703 - val_loss: 0.4957 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7334 - loss: 0.5423\n",
            "Epoch 3: val_loss improved from 0.49566 to 0.43304, saving model to /content/model/sentiment_model_best.keras\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 2s/step - accuracy: 0.7334 - loss: 0.5423 - val_accuracy: 0.8030 - val_loss: 0.4330 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7501 - loss: 0.5167\n",
            "Epoch 4: val_loss improved from 0.43304 to 0.41364, saving model to /content/model/sentiment_model_best.keras\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 2s/step - accuracy: 0.7501 - loss: 0.5166 - val_accuracy: 0.8155 - val_loss: 0.4136 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8000 - loss: 0.4515\n",
            "Epoch 5: val_loss improved from 0.41364 to 0.39104, saving model to /content/model/sentiment_model_best.keras\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 2s/step - accuracy: 0.8000 - loss: 0.4515 - val_accuracy: 0.8305 - val_loss: 0.3910 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8038 - loss: 0.4328\n",
            "Epoch 6: val_loss improved from 0.39104 to 0.35309, saving model to /content/model/sentiment_model_best.keras\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 2s/step - accuracy: 0.8038 - loss: 0.4328 - val_accuracy: 0.8512 - val_loss: 0.3531 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8205 - loss: 0.4016\n",
            "Epoch 7: val_loss did not improve from 0.35309\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 2s/step - accuracy: 0.8205 - loss: 0.4016 - val_accuracy: 0.8432 - val_loss: 0.3918 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8342 - loss: 0.3827\n",
            "Epoch 8: val_loss improved from 0.35309 to 0.33548, saving model to /content/model/sentiment_model_best.keras\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 2s/step - accuracy: 0.8342 - loss: 0.3827 - val_accuracy: 0.8550 - val_loss: 0.3355 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8363 - loss: 0.3765\n",
            "Epoch 9: val_loss did not improve from 0.33548\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 2s/step - accuracy: 0.8363 - loss: 0.3765 - val_accuracy: 0.8525 - val_loss: 0.3416 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8412 - loss: 0.3617\n",
            "Epoch 10: val_loss improved from 0.33548 to 0.31809, saving model to /content/model/sentiment_model_best.keras\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m436s\u001b[0m 2s/step - accuracy: 0.8412 - loss: 0.3617 - val_accuracy: 0.8645 - val_loss: 0.3181 - learning_rate: 0.0010\n",
            "Restoring model weights from the end of the best epoch: 10.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_model_path = os.path.join(MODEL_DIR, \"sentiment_model.keras\")\n",
        "model.save(final_model_path)\n",
        "print(\"Saved model to\", final_model_path)\n",
        "\n",
        "# also copy to Drive if mounted\n",
        "try:\n",
        "    import shutil\n",
        "    dst = os.path.join(DRIVE_OUT, \"model\")\n",
        "    os.makedirs(dst, exist_ok=True)\n",
        "    shutil.copy(final_model_path, dst)\n",
        "    shutil.copy(os.path.join(MODEL_DIR, \"tokenizer.pkl\"), dst)\n",
        "    print(\"Copied model+tokenizer to Drive at\", dst)\n",
        "except Exception as e:\n",
        "    print(\"Copy to Drive skipped or failed:\", e)\n",
        "\n",
        "# Evaluate\n",
        "probs = model.predict(X_test_pad, batch_size=1024)\n",
        "preds = np.argmax(probs, axis=1)\n",
        "acc = accuracy_score(y_test, preds)\n",
        "print(\"Test Accuracy: {:.4f}\".format(acc))\n",
        "print(\"\\nClassification report:\\n\")\n",
        "print(classification_report(y_test, preds, digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stTWdAIYr42P",
        "outputId": "a40e6135-5316-4ef6-bf15-4310d8487e7c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to /content/model/sentiment_model.keras\n",
            "Copy to Drive skipped or failed: name 'DRIVE_OUT' is not defined\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 5s/step\n",
            "Test Accuracy: 0.8597\n",
            "\n",
            "Classification report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8849    0.8270    0.8550      5000\n",
            "           1     0.8376    0.8924    0.8641      5000\n",
            "\n",
            "    accuracy                         0.8597     10000\n",
            "   macro avg     0.8612    0.8597    0.8595     10000\n",
            "weighted avg     0.8612    0.8597    0.8595     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load tokenizer and model to confirm inference works\n",
        "with open(os.path.join(MODEL_DIR, \"tokenizer.pkl\"), \"rb\") as f:\n",
        "    tok = pickle.load(f)\n",
        "\n",
        "m = tf.keras.models.load_model(os.path.join(MODEL_DIR, \"sentiment_model.keras\"))\n",
        "\n",
        "def predict_text(text):\n",
        "    clean = custom_standardization(text)\n",
        "    seq = tok.texts_to_sequences([clean])\n",
        "    pad = pad_sequences(seq, maxlen=MAX_LEN, padding='post')\n",
        "    prob = m.predict(pad)[0]\n",
        "    return {\"negative_prob\": float(prob[0]), \"positive_prob\": float(prob[1]),\n",
        "            \"sentiment\": \"Positive\" if prob[1] > 0.5 else \"Negative\"}\n",
        "\n",
        "# test\n",
        "print(predict_text(\"This was the best movie I have ever seen! I loved it.\"))\n",
        "print(predict_text(\"I hated this movie. It was boring and a waste of time.\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owcHqBdhsXol",
        "outputId": "d81b9e10-e342-454c-8165-660caaf4b3c0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "{'negative_prob': 0.02728801779448986, 'positive_prob': 0.972711980342865, 'sentiment': 'Positive'}\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
            "{'negative_prob': 0.9676733016967773, 'positive_prob': 0.032326724380254745, 'sentiment': 'Negative'}\n"
          ]
        }
      ]
    }
  ]
}